<post title="Linear Algebra Notes">
  <h1>Linear System</h1>

  <p>A system of equations looks like this:</p>

  <math>
    \begin{cases}
      2x + 3y + 6z = 3\\
      x â€“ y + 3z = 0\\
      x + -4y + 5z = 1
    \end{cases}
  </math>

  <p>And it can be represented as a matrix, like this (This is the <b>augmented
  matrix</b>. Removing the rightmost column would make it the <b>coefficients
  table</b>):</p>

  <math><![CDATA[
    \begin{bmatrix}
      2 & 3 & 6 & 3\\
      1 & -1 & 3 & 0\\
      1 & -4 & 5 & 1\\
    \end{bmatrix}
    ]]>
  </math>

  <p>A linear system is <b>inconsistent</b> if it has no solution, and
  <b>consistent</b> if it has either one solution or infinite solutions.</p>

  <h1>Gaussian Elimination</h1>

  <p>An <b>elementary matrix</b> is the result of applying an elementary row
  operation to the identity matrix.</p>

  <ul>
    <b>Elementary Row Operations</b>
    <li>Interchange: Replacing one row with the other.</li>
    <li>Multiplication: Each element in a row is multiplied by a non-zero $k$.</li>
    <li>Addition: Replacing one row with the sum of itself and a multiplication
    of another.</li>
  </ul>

  <p>Gaussian elimination consists of applying these operations to the matrices
  (Formally, applying here means multiplying the matrix by the elementary matrix
  associated to that transformation) to turn the matrix into <b>row echelon
  form</b>, where we can tell whether it's consistent or not, and if it is, find
  its solution set.

  A matrix is in row echelon form when:</p>

  <ul>
    <li>Every all-zero row is at the bottom of the matrix.</li>
    <li>The leading coefficient of a non-zero row must be to the left of the one
    below it.</li>
    <li>All entries below a leading coefficient are zero.</li>
  </ul>

  <p>(The <b>leading coefficient</b> of a matrix is the first non-zero entry in a row).

  Basically, it looks like this:</p>

  <math><![CDATA[
    \begin{bmatrix}
      1 & 7 & 2 & 3\\
      0 & 4 & 1 & 2\\
      0 & 0 & 6 & 9 \\
    \end{bmatrix}
    ]]>
  </math>

  <p>After performing Gaussian Elimination, if two matrices have the same row
  echelon form are said to be <b>row equivalent</b> (More formally, there exists
  a sequence of operations that transforms one to the other.</p>

  <h2>Testing Consistency</h2>

  <p>If, after Gaussian Elimination, a system has an equality like $c = k,
  \forall c, k \in R: c \neq k$, then the system is inconsistent.

  If all the variables have a fixed value, the solution is unique. Otherwise, if
  there is at least one free variable, there are infinite solutions.</p>

  <h1>Vectors</h1>

  <p>A vector, for the purposes of this course, is a matrix with a single
  column.</p>

  <h1>Linear Combinations</h1>

  <p>The vector $y = c_1v_1 + ... + c_nv_n$ is a <b>linear combination</b> of
  the vectors $\vflist{v}$ and weights $\vflist{c}$. The <b>vector equation</b>
  $x_1v_1, x_2v_2, ..., x_nv_n = y$ has the same solution as the system whose
  matrix is $[\vflist{v}]$.

  The set of linear combinations that can be generated from the vectors
  $\vflist{v}$ is denoted $\Span{\vflist{v}}$.

  The spanned set includes the zero vector, and every multiple of every vector
  $\vflist{v}$.

  The vector equation $x_1v_1, x_2v_2, ..., x_nv_n = y$ is equivalent to the
  matrix equation $Ax = y$, where $A$ is the matrix of vectors $[\vflist{v}]$
  and $x$ is the vector of weights. Asking whether $y \in \Span{\vflist{v}}$ is
  equivalent to asking whether $Ax = y$ is consistent.</p>

  <h2>Solutions</h2>

  <p>A system that can be written as $Ax = 0$ is <b>homogenous</b>. The solution
  $x = 0$, which this type of system always has, is the <b>trivial
  solution</b>. The system has a <b>non-trivial solution</b> if it has at least
  one free variable.</p>

  <h2>Linear Independence</h2>

  <p>A set of vectors $\vflist{v}$ are <b>linearly independent</b> if the vector
  equation $x_1v_1, x_2v_2, ..., x_nv_n = 0$ has only the trivial solution.

  If the equation $Ax = 0$ has only the trivial solution, then the columns of
  $A$ are linearly independent.

  A set of vectors is linearly dependent when <b>at least one</b> of the vectors
  is a multiple of any of the others.

  If a set has more vectors than the number of entries in each vector, then it's
  linearly dependent.

  If a set contains the zero vector, it's linearly dependent.</p>

  <h2>Linear Transformations</h2>

  <p>A transformation is a function $\fn{T}{R^n}{R^m}$.

  A transformation is <b>linear</b> if:</p>

  <ul>
    <li>$T(u+v) = T(u) + T(v)$</li>
    <li>$T(cu) = cT(u)$</li>
  </ul>

  <p>Every matrix transformation $T(x) = Ax$ is linear.</p>

  <h3>Matrix of a Linear Transformation</h3>

  <p>Every linear transformation is of the form $T(x) = Ax$, and $A = [T(I_1)
  T(I_2) ... T(I_n)]$, where $I_i$ is the $i$-th column of the identity
  matrix.

  T is surjective if the columns of its matrix span the codomain.

  T is injective if the columns of its matrix are linearly independent.</p>

  <h1>Matrix Algebra</h1>

  <p>A diagonal matrix has all zeros outside the diagonal.

  The sum of two matrices is just the matrix of sums of corresponding
  entries.

  The <b>transposition</b> of a matrix, $A^T$, is the result of flipping
  everything along the diagonal. Properties: $(A+B)^T = A^T + B^T$, $(AB)^T =
  B^TA^T$.</p>

  <h3>Inverse</h3>

  <p>A matrix $A$ is <b>invertible</b> if $\exists C : AC = I$.

  To find an inverse, perform Gaussian elimination on $[A I]$ until it becomes
  $[I A^-1]$. If that's not possible, it's not invertible.

  If A is invertible, A is linearly independent, and $A^T$ is invertible.

  The inverse of a linear transformation $T(x) = Ax$ is $S(x) = A^-1x$.</p>
</post>
